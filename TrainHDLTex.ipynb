{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "HDLTex: Hierarchical Deep Learning for Text Classification\n",
    "load and tokenization module the input strings for deep learning model\n",
    "\n",
    "* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    "* Last Update: Oct 26, 2018\n",
    "* This file is part of  HDLTex project, University of Virginia.\n",
    "* Free to use, change, share and distribute source code of RMDL\n",
    "* Refrenced paper : HDLTex: Hierarchical Deep Learning for Text Classification\n",
    "* Link: https://doi.org/10.1109/ICMLA.2017.0-134\n",
    "* Comments and Error: email: kk7nc@virginia.edu\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \"\"\"\n",
    "    cleaning spaces, html tags, etc\n",
    "    parameters: (string) text input to clean\n",
    "    return: (string) clean_text\n",
    "    \"\"\"\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.replace(\"[\", \" \")\n",
    "    text = text.replace(\",\", \" \")\n",
    "    text = text.replace(\"]\", \" \")\n",
    "    text = text.replace(\"(\", \" \")\n",
    "    text = text.replace(\")\", \" \")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"=\", \"\")\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
    "    ]\n",
    "    for rule in rules:\n",
    "        for (k, v) in rule.items():\n",
    "            regex = re.compile(k)\n",
    "            text = regex.sub(v, text)\n",
    "        text = text.rstrip()\n",
    "        text = text.strip()\n",
    "    clean_text = text.lower()\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def loadData_Tokenizer(MAX_NB_WORDS,MAX_SEQUENCE_LENGTH):\n",
    "    fname = os.path.join(\"WOS\",\"X.txt\")\n",
    "    fnamek = os.path.join(\"WOS\",\"YL1.txt\")\n",
    "    fnameL2 = os.path.join(\"WOS\",\"YL2.txt\")\n",
    "\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "        content = [clean_str(x) for x in content]\n",
    "    content = np.array(content)\n",
    "    with open(fnamek) as fk:\n",
    "        contentk = fk.readlines()\n",
    "    contentk = [x.strip() for x in contentk]\n",
    "    with open(fnameL2) as fk:\n",
    "        contentL2 = fk.readlines()\n",
    "        contentL2 = [x.strip() for x in contentL2]\n",
    "    Label = np.matrix(contentk, dtype=int)\n",
    "    Label = np.transpose(Label)\n",
    "    number_of_classes_L1 = np.max(Label)+1 #number of classes in Level 1\n",
    "\n",
    "    Label_L2 = np.matrix(contentL2, dtype=int)\n",
    "    Label_L2 = np.transpose(Label_L2)\n",
    "    np.random.seed(7)\n",
    "\n",
    "    Label = np.column_stack((Label, Label_L2))\n",
    "\n",
    "    number_of_classes_L2 = np.zeros(number_of_classes_L1,dtype=int) #number of classes in Level 2 that is 1D array with size of (number of classes in level one,1)\n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(content)\n",
    "    sequences = tokenizer.texts_to_sequences(content)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    content = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    indices = np.arange(content.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    content = content[indices]\n",
    "    Label = Label[indices]\n",
    "    print(content.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(content, Label, test_size=0.2, random_state=0)\n",
    "\n",
    "    L2_Train = []\n",
    "    L2_Test = []\n",
    "    content_L2_Train = []\n",
    "    content_L2_Test = []\n",
    "    '''\n",
    "    crewate #L1 number of train and test sample for level two of Hierarchical Deep Learning models\n",
    "    '''\n",
    "    for i in range(0, number_of_classes_L1):\n",
    "        L2_Train.append([])\n",
    "        L2_Test.append([])\n",
    "        content_L2_Train.append([])\n",
    "        content_L2_Test.append([])\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        X_test= np.array(X_test)\n",
    "    for i in range(0, X_train.shape[0]):\n",
    "        L2_Train[y_train[i, 0]].append(y_train[i, 1])\n",
    "        number_of_classes_L2[y_train[i, 0]] = max(number_of_classes_L2[y_train[i, 0]],(y_train[i, 1]+1))\n",
    "        content_L2_Train[y_train[i, 0]].append(X_train[i])\n",
    "\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        L2_Test[y_test[i, 0]].append(y_test[i, 1])\n",
    "        content_L2_Test[y_test[i, 0]].append(X_test[i])\n",
    "\n",
    "    for i in range(0, number_of_classes_L1):\n",
    "        L2_Train[i] = np.array(L2_Train[i])\n",
    "        L2_Test[i] = np.array(L2_Test[i])\n",
    "        content_L2_Train[i] = np.array(content_L2_Train[i])\n",
    "        content_L2_Test[i] = np.array(content_L2_Test[i])\n",
    "\n",
    "    embeddings_index = {}\n",
    "    '''\n",
    "    For CNN and RNN, we used the text vector-space models using $100$ dimensions as described in Glove. A vector-space model is a mathematical mapping of the word space\n",
    "    '''\n",
    "    Glove_path = os.path.join(\"GLOVE\", 'glove.6B.100d.txt')\n",
    "    print(Glove_path)\n",
    "    f = open(Glove_path, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            print(\"Warnning\"+str(values)+\" in\" + str(line))\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, y_train, X_test, y_test, content_L2_Train, L2_Train, content_L2_Test, L2_Test, number_of_classes_L2,word_index,embeddings_index,number_of_classes_L1)\n",
    "\n",
    "def loadData():\n",
    "    fname = os.path.join(\"WOS\",\"X.txt\")\n",
    "    fnamek = os.path.join(\"WOS\",\"YL1.txt\")\n",
    "    fnameL2 = os.path.join(\"WOS\",\"YL2.txt\")\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "        content = [text_cleaner(x) for x in content]\n",
    "    with open(fnamek) as fk:\n",
    "        contentk = fk.readlines()\n",
    "    contentk = [x.strip() for x in contentk]\n",
    "    with open(fnameL2) as fk:\n",
    "        contentL2 = fk.readlines()\n",
    "        contentL2 = [x.strip() for x in contentL2]\n",
    "    Label = np.matrix(contentk, dtype=int)\n",
    "    Label = np.transpose(Label)\n",
    "    number_of_classes_L1 = np.max(Label)+1  # number of classes in Level 1\n",
    "\n",
    "    Label_L2 = np.matrix(contentL2, dtype=int)\n",
    "    Label_L2 = np.transpose(Label_L2)\n",
    "    np.random.seed(7)\n",
    "    print(Label.shape)\n",
    "    print(Label_L2.shape)\n",
    "    Label = np.column_stack((Label, Label_L2))\n",
    "\n",
    "    number_of_classes_L2 = np.zeros(number_of_classes_L1,dtype=int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(content, Label, test_size=0.2,random_state= 0)\n",
    "\n",
    "    vectorizer_x = CountVectorizer()\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "\n",
    "    L2_Train = []\n",
    "    L2_Test = []\n",
    "    content_L2_Train = []\n",
    "    content_L2_Test = []\n",
    "\n",
    "    for i in range(0, number_of_classes_L1):\n",
    "        L2_Train.append([])\n",
    "        L2_Test.append([])\n",
    "        content_L2_Train.append([])\n",
    "        content_L2_Test.append([])\n",
    "\n",
    "    for i in range(0, X_train.shape[0]):\n",
    "        L2_Train[y_train[i, 0]].append(y_train[i, 1])\n",
    "        number_of_classes_L2[y_train[i, 0]] = max(number_of_classes_L2[y_train[i, 0]],(y_train[i, 1]+1))\n",
    "        content_L2_Train[y_train[i, 0]].append(X_train[i])\n",
    "\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        L2_Test[y_test[i, 0]].append(y_test[i, 1])\n",
    "        content_L2_Test[y_test[i, 0]].append(X_test[i])\n",
    "\n",
    "    for i in range(0, number_of_classes_L1):\n",
    "        L2_Train[i] = np.array(L2_Train[i])\n",
    "        L2_Test[i] = np.array(L2_Test[i])\n",
    "        content_L2_Train[i] = np.array(content_L2_Train[i])\n",
    "        content_L2_Test[i] = np.array(content_L2_Test[i])\n",
    "    return (X_train,y_train,X_test,y_test,content_L2_Train,L2_Train,content_L2_Test,L2_Test,number_of_classes_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "HDLTex: Hierarchical Deep Learning for Text Classification\n",
    "module for building of different deep learning models (DNN, RNN, CNN)\n",
    "\n",
    "* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    "* Last Update: Oct 26, 2018\n",
    "* This file is part of  HDLTex project, University of Virginia.\n",
    "* Free to use, change, share and distribute source code of RMDL\n",
    "* Refrenced paper : HDLTex: Hierarchical Deep Learning for Text Classification\n",
    "* Link: https://doi.org/10.1109/ICMLA.2017.0-134\n",
    "* Comments and Error: email: kk7nc@virginia.edu\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, SimpleRNN\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def buildModel_DNN(Shape, nClasses, nLayers=3,Number_Node=100, dropout=0.5):\n",
    "    '''\n",
    "    buildModel_DNN(nFeatures, nClasses, nLayers=3,Numberof_NOde=100, dropout=0.5)\n",
    "    Build Deep neural networks (Multi-layer perceptron) Model for text classification\n",
    "    Shape is input feature space\n",
    "    nClasses is number of classes\n",
    "    nLayers is number of hidden Layer\n",
    "    Number_Node is number of unit in each hidden layer\n",
    "    dropout is dropout value for solving overfitting problem\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(Number_Node, input_dim=Shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(Number_Node, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def buildModel_RNN(word_index, embeddings_index, nClasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):\n",
    "    '''\n",
    "    def buildModel_RNN(word_index, embeddings_index, nClasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):\n",
    "    word_index in word index ,\n",
    "    embeddings_index is embeddings index, look at data_helper.py\n",
    "    nClasses is number of classes,\n",
    "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
    "    EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
    "    output: RNN model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    model.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def buildModel_CNN(word_index,embeddings_index,nClasses,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,Complexity=1):\n",
    "    '''\n",
    "    def buildModel_CNN(word_index,embeddings_index,nClasses,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,Complexity=0):\n",
    "    word_index in word index ,\n",
    "    embeddings_index is embeddings index, look at data_helper.py\n",
    "    nClasses is number of classes,\n",
    "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
    "    EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
    "    Complexity we have two different CNN model as follows\n",
    "    Complexity=0 is simple CNN with 3 hidden layer\n",
    "    Complexity=2 is more complex model of CNN with filter_length of [3, 4, 5, 6, 7]\n",
    "    return: (CNN model) model\n",
    "    '''\n",
    "    if Complexity==0:\n",
    "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=True)\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        x = Conv1D(256, 5, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(5)(x)\n",
    "        x = Conv1D(256, 5, activation='relu')(x)\n",
    "        x = MaxPooling1D(5)(x)\n",
    "        x = Conv1D(256, 5, activation='relu')(x)\n",
    "        x = MaxPooling1D(35)(x)  # global max pooling\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        preds = Dense(nClasses, activation='softmax')(x)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "    else:\n",
    "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=True)\n",
    "\n",
    "        convs = []\n",
    "        filter_sizes = [3, 4, 5, 6, 7]\n",
    "\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        for fsz in filter_sizes:\n",
    "            l_conv = Conv1D(128, filter_length=fsz, activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(5)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "\n",
    "        l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "        l_cov1 = Conv1D(128, 5, activation='relu')(l_merge)\n",
    "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "        l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "        l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "        l_flat = Flatten()(l_pool2)\n",
    "        l_dense = Dense(128, activation='relu')(l_flat)\n",
    "        preds = Dense(nClasses, activation='softmax')(l_dense)\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "HDLTex: Hierarchical Deep Learning for Text Classification\n",
    "script to run main fucntion and create hierarchical structure\n",
    "\n",
    "* Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    "* Last Update: Oct 26, 2018\n",
    "* This file is part of  HDLTex project, University of Virginia.\n",
    "* Free to use, change, share and distribute source code of RMDL\n",
    "* Refrenced paper : HDLTex: Hierarchical Deep Learning for Text Classification\n",
    "* Link: https://doi.org/10.1109/ICMLA.2017.0-134\n",
    "* Comments and Error: email: kk7nc@virginia.edu\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "\n",
    "MEMORY_MB_MAX = 1600000 # maximum memory you can use\n",
    "MAX_SEQUENCE_LENGTH = 500 # Maximum sequance lenght 500 words\n",
    "MAX_NB_WORDS = 55000 # Maximum number of unique words\n",
    "EMBEDDING_DIM = 100 #embedding dimension you can change it to {25, 100, 150, and 300} but need to change glove version\n",
    "batch_size_L1 = 64 # batch size in Level 1\n",
    "batch_size_L2 = 64 # batch size in Level 2\n",
    "epochs = 1\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "'''\n",
    "location of input data in two ways\n",
    "1: Tokenizer that is using GLOVE\n",
    "1: loadData that is using couting words or tf-idf\n",
    "'''\n",
    "\n",
    "X_train, y_train, X_test, y_test, content_L2_Train, L2_Train, content_L2_Test, L2_Test, number_of_classes_L2,word_index, embeddings_index,number_of_classes_L1 = loadData_Tokenizer(MAX_NB_WORDS,MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train_DNN, y_train_DNN, X_test_DNN, y_test_DNN, content_L2_Train_DNN, L2_Train_DNN, content_L2_Test_DNN, L2_Test_DNN, number_of_classes_L2_DNN = loadData()\n",
    "print(\"Loading Data is Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################DNN Level 1########################\n",
    "print('Create model of DNN')\n",
    "model = buildModel_DNN(X_train_DNN.shape[1], number_of_classes_L1, 8, 64, dropout=0.25)\n",
    "model.fit(X_train_DNN, y_train_DNN[:, 0],\n",
    "            validation_data=(X_test_DNN, y_test_DNN[:, 0]),\n",
    "            epochs=epochs,\n",
    "            verbose=2,\n",
    "            batch_size=batch_size_L1)\n",
    "model.save(f'Modelo_Lvl1_DNN.keras')\n",
    "#######################CNN Level 1########################\n",
    "print('Create model of CNN')\n",
    "model = buildModel_CNN(word_index, embeddings_index,number_of_classes_L1,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1)\n",
    "model.fit(X_train, y_train[:,0],\n",
    "            validation_data=(X_test, y_test[:,0]),\n",
    "            epochs=epochs,\n",
    "            verbose=2,\n",
    "            batch_size=batch_size_L1)\n",
    "model.save(f'Modelo_Lvl1_CNN.keras')\n",
    "#######################RNN Level 1########################\n",
    "print('Create model of RNN')\n",
    "model = buildModel_RNN(word_index, embeddings_index,number_of_classes_L1,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)\n",
    "model.fit(X_train, y_train[:,0],\n",
    "            validation_data=(X_test, y_test[:,0]),\n",
    "            epochs=epochs,\n",
    "            verbose=2,\n",
    "            batch_size=batch_size_L1)\n",
    "model.save(f'Modelo_Lvl1_RNN.keras')\n",
    "\n",
    "######################DNN Level 2################################\n",
    "HDLTex = [] # Level 2 models is list of Deep Structure\n",
    "for i in range(0, number_of_classes_L1):\n",
    "    print('Create Sub model of ',i)\n",
    "    HDLTex.append(Sequential())\n",
    "    HDLTex[i] = buildModel_DNN(content_L2_Train_DNN[i].shape[1], number_of_classes_L2_DNN[i],2, 1024, dropout=0.5)\n",
    "    HDLTex[i].fit(content_L2_Train_DNN[i], L2_Train_DNN[i],\n",
    "            validation_data=(content_L2_Test_DNN[i], L2_Test_DNN[i]),\n",
    "            epochs=epochs,\n",
    "            verbose=2,\n",
    "            batch_size=batch_size_L2)\n",
    "for idx, model in enumerate(HDLTex):\n",
    "  model.save(f'Modelo_Lvl2_DNN_{idx}.keras')\n",
    "######################CNN Level 2################################\n",
    "HDLTex = [] # Level 2 models is list of Deep Structure\n",
    "for i in range(0, number_of_classes_L1):\n",
    "    print('Create Sub model of ', i)\n",
    "    HDLTex.append(Sequential())\n",
    "    HDLTex[i] = buildModel_CNN(word_index, embeddings_index,number_of_classes_L2[i],MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1)\n",
    "    HDLTex[i].fit(content_L2_Train[i], L2_Train[i],\n",
    "                    validation_data=(content_L2_Test[i], L2_Test[i]),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    batch_size=batch_size_L2)\n",
    "for idx, model in enumerate(HDLTex):\n",
    "  model.save(f'Modelo_Lvl2_CNN_{idx}.keras')\n",
    "######################RNN Level 2################################\n",
    "HDLTex = [] # Level 2 models is list of Deep Structure\n",
    "for i in range(0, number_of_classes_L1):\n",
    "    print('Create Sub model of ', i)\n",
    "    HDLTex.append(Sequential())\n",
    "    HDLTex[i] = buildModel_RNN(word_index, embeddings_index,number_of_classes_L2[i],MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)\n",
    "    HDLTex[i].fit(content_L2_Train[i], L2_Train[i],\n",
    "                    validation_data=(content_L2_Test[i], L2_Test[i]),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    batch_size=batch_size_L2)\n",
    "for idx, model in enumerate(HDLTex):\n",
    "  model.save(f'Modelo_Lvl2_RNN_{idx}.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_PLN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
